+ ENGINE=vllm
+ export VLLM_ATTENTION_BACKEND=XFORMERS
+ VLLM_ATTENTION_BACKEND=XFORMERS
+ num_cpus_per_env_worker=0.1
+ train_data_size=4
+ val_data_size=4
+ group_size=4
+ python3 -m examples.data_preprocess.prepare --mode text --train_data_size 4 --val_data_size 4
num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.
WARNING:2025-12-09 14:17:21,569:num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.
num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.
WARNING:2025-12-09 14:17:21,620:num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.
processing data for mode: text
Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1768.26ba/s]
Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2953.74ba/s]
Exception ignored in: <function ResourceTracker.__del__ at 0x7f824f5249a0>
Traceback (most recent call last):
  File "/users/3/peng0504/.conda/envs/verl-agent/lib/python3.12/site-packages/multiprocess/resource_tracker.py", line 80, in __del__
  File "/users/3/peng0504/.conda/envs/verl-agent/lib/python3.12/site-packages/multiprocess/resource_tracker.py", line 89, in _stop
  File "/users/3/peng0504/.conda/envs/verl-agent/lib/python3.12/site-packages/multiprocess/resource_tracker.py", line 102, in _stop_locked
AttributeError: '_thread.RLock' object has no attribute '_recursion_count'
+ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo data.train_files=/users/3/peng0504/data/verl-agent/text/train.parquet data.val_files=/users/3/peng0504/data/verl-agent/text/test.parquet data.train_batch_size=4 data.val_batch_size=4 data.max_prompt_length=2048 data.max_response_length=512 data.filter_overlong_prompts=True data.truncation=error data.return_raw_chat=True actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=4 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.01 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.rollout.tensor_model_parallel_size=2 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.enable_chunked_prefill=False actor_rollout_ref.rollout.enforce_eager=False actor_rollout_ref.rollout.free_cache_engine=False actor_rollout_ref.rollout.val_kwargs.temperature=0.4 actor_rollout_ref.rollout.val_kwargs.do_sample=True actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=1 actor_rollout_ref.ref.fsdp_config.param_offload=True actor_rollout_ref.actor.use_invalid_action_penalty=True actor_rollout_ref.actor.invalid_action_penalty_coef=0.1 algorithm.use_kl_in_reward=False env.env_name=alfworld/AlfredTWEnv env.seed=0 env.max_steps=50 env.rollout.n=4 env.resources_per_worker.num_cpus=0.1 trainer.critic_warmup=0 'trainer.logger=[console,wandb]' trainer.project_name=verl_agent_alfworld trainer.experiment_name=grpo_qwen2.5_0.5b trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.save_freq=-1 trainer.test_freq=5 trainer.total_epochs=150 trainer.val_before_train=True
2025-12-09 14:17:38,768	INFO worker.py:1942 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(TaskRunner pid=3489667)[0m   0%|          | 0/8810 [00:00<?, ?it/s]
[36m(TaskRunner pid=3489667)[0m   1%|â–         | 124/8810 [00:00<00:07, 1239.18it/s]
[36m(TaskRunner pid=3489667)[0m   3%|â–Ž         | 253/8810 [00:00<00:06, 1266.85it/s]
[36m(TaskRunner pid=3489667)[0m   5%|â–         | 438/8810 [00:00<00:05, 1531.86it/s]
[36m(TaskRunner pid=3489667)[0m   7%|â–‹         | 592/8810 [00:00<00:06, 1368.69it/s]
[36m(TaskRunner pid=3489667)[0m   8%|â–Š         | 732/8810 [00:00<00:06, 1298.73it/s]
[36m(TaskRunner pid=3489667)[0m  10%|â–‰         | 864/8810 [00:00<00:06, 1300.06it/s]
[36m(TaskRunner pid=3489667)[0m  11%|â–ˆâ–        | 996/8810 [00:00<00:08, 893.81it/s] 
[36m(TaskRunner pid=3489667)[0m  13%|â–ˆâ–Ž        | 1118/8810 [00:01<00:07, 965.77it/s]
[36m(TaskRunner pid=3489667)[0m  14%|â–ˆâ–        | 1229/8810 [00:01<00:07, 985.42it/s]
[36m(TaskRunner pid=3489667)[0m  15%|â–ˆâ–Œ        | 1339/8810 [00:01<00:07, 1013.60it/s]
[36m(TaskRunner pid=3489667)[0m  17%|â–ˆâ–‹        | 1457/8810 [00:01<00:06, 1058.30it/s]
[36m(TaskRunner pid=3489667)[0m  18%|â–ˆâ–Š        | 1574/8810 [00:01<00:06, 1087.39it/s]
[36m(TaskRunner pid=3489667)[0m  19%|â–ˆâ–‰        | 1694/8810 [00:01<00:06, 1117.77it/s]
[36m(TaskRunner pid=3489667)[0m  21%|â–ˆâ–ˆ        | 1809/8810 [00:01<00:06, 1018.28it/s]
[36m(TaskRunner pid=3489667)[0m  22%|â–ˆâ–ˆâ–       | 1915/8810 [00:01<00:07, 946.28it/s] 
[36m(TaskRunner pid=3489667)[0m  23%|â–ˆâ–ˆâ–Ž       | 2013/8810 [00:01<00:07, 927.39it/s]
[36m(TaskRunner pid=3489667)[0m  24%|â–ˆâ–ˆâ–       | 2108/8810 [00:02<00:07, 892.67it/s]
[36m(TaskRunner pid=3489667)[0m  25%|â–ˆâ–ˆâ–Œ       | 2210/8810 [00:02<00:07, 923.88it/s]
[36m(TaskRunner pid=3489667)[0m  26%|â–ˆâ–ˆâ–‹       | 2328/8810 [00:02<00:06, 991.98it/s]
[36m(TaskRunner pid=3489667)[0m  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3770/8810 [00:02<00:01, 4766.76it/s]
[36m(TaskRunner pid=3489667)[0m  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4270/8810 [00:02<00:02, 2168.15it/s]
[36m(TaskRunner pid=3489667)[0m  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4648/8810 [00:03<00:02, 1652.64it/s]
[36m(TaskRunner pid=3489667)[0m  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 4942/8810 [00:03<00:03, 1136.59it/s]
[36m(TaskRunner pid=3489667)[0m  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5163/8810 [00:04<00:03, 1117.93it/s]
[36m(TaskRunner pid=3489667)[0m  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 5349/8810 [00:04<00:03, 1067.78it/s]
[36m(TaskRunner pid=3489667)[0m  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5506/8810 [00:04<00:02, 1118.36it/s]
[36m(TaskRunner pid=3489667)[0m  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5659/8810 [00:04<00:03, 984.81it/s] 
[36m(TaskRunner pid=3489667)[0m  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 5786/8810 [00:04<00:04, 712.19it/s]
[36m(TaskRunner pid=3489667)[0m  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 5885/8810 [00:05<00:04, 661.01it/s]
[36m(TaskRunner pid=3489667)[0m  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6011/8810 [00:05<00:03, 745.12it/s]
[36m(TaskRunner pid=3489667)[0m  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6142/8810 [00:05<00:03, 838.29it/s]
[36m(TaskRunner pid=3489667)[0m  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6335/8810 [00:05<00:02, 1049.60it/s]
[36m(TaskRunner pid=3489667)[0m  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6510/8810 [00:05<00:01, 1198.84it/s]
[36m(TaskRunner pid=3489667)[0m  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6654/8810 [00:05<00:02, 1007.76it/s]
[36m(TaskRunner pid=3489667)[0m  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 6776/8810 [00:06<00:02, 713.76it/s] 
[36m(TaskRunner pid=3489667)[0m  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 6873/8810 [00:06<00:02, 716.74it/s]
[36m(TaskRunner pid=3489667)[0m  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 6963/8810 [00:06<00:02, 718.32it/s]
[36m(TaskRunner pid=3489667)[0m  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7175/8810 [00:06<00:01, 1000.30it/s]
[36m(TaskRunner pid=3489667)[0m  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 7360/8810 [00:06<00:01, 1189.57it/s]
[36m(TaskRunner pid=3489667)[0m  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 7500/8810 [00:06<00:01, 1123.80it/s]
[36m(TaskRunner pid=3489667)[0m  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 7628/8810 [00:06<00:01, 1018.92it/s]
[36m(TaskRunner pid=3489667)[0m  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7742/8810 [00:06<00:01, 1000.81it/s]
[36m(TaskRunner pid=3489667)[0m  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 7850/8810 [00:07<00:01, 728.60it/s] 
[36m(TaskRunner pid=3489667)[0m  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 7952/8810 [00:07<00:01, 785.18it/s]
[36m(TaskRunner pid=3489667)[0m  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8048/8810 [00:07<00:00, 822.84it/s]
[36m(TaskRunner pid=3489667)[0m  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8141/8810 [00:07<00:00, 840.06it/s]
[36m(TaskRunner pid=3489667)[0m  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8233/8810 [00:07<00:00, 843.02it/s]
[36m(TaskRunner pid=3489667)[0m  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8338/8810 [00:07<00:00, 894.92it/s]
[36m(TaskRunner pid=3489667)[0m  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 8433/8810 [00:07<00:00, 889.76it/s]
[36m(TaskRunner pid=3489667)[0m  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 8526/8810 [00:07<00:00, 855.66it/s]
[36m(TaskRunner pid=3489667)[0m  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 8614/8810 [00:08<00:00, 808.32it/s]
[36m(TaskRunner pid=3489667)[0m  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 8697/8810 [00:08<00:00, 786.57it/s]
[36m(TaskRunner pid=3489667)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8783/8810 [00:08<00:00, 805.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8810/8810 [00:08<00:00, 1056.75it/s]
[36m(TaskRunner pid=3489667)[0m   0%|          | 0/494 [00:00<?, ?it/s]
[36m(TaskRunner pid=3489667)[0m  19%|â–ˆâ–‰        | 94/494 [00:00<00:00, 925.54it/s]
[36m(TaskRunner pid=3489667)[0m  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 252/494 [00:00<00:00, 1291.33it/s]
[36m(TaskRunner pid=3489667)[0m  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 382/494 [00:00<00:00, 1231.02it/s]
[36m(TaskRunner pid=3489667)[0m 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 494/494 [00:00<00:00, 1120.17it/s]
[36m(TaskRunner pid=3489667)[0m Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 4 examples [00:00, 149.94 examples/s]
[36m(TaskRunner pid=3489667)[0m Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(TaskRunner pid=3489667)[0m WARNING:2025-12-09 14:18:14,718:Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(TaskRunner pid=3489667)[0m Filtering prompts longer than 2048 tokens (num_proc=1):   0%|          | 0/4 [00:00<?, ? examples/s]
[36m(TaskRunner pid=3489667)[0m Filtering prompts longer than 2048 tokens (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  8.97 examples/s]
[36m(TaskRunner pid=3489667)[0m Filtering prompts longer than 2048 tokens (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  7.23 examples/s]
[36m(TaskRunner pid=3489667)[0m Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 4 examples [00:00, 447.11 examples/s]
[36m(TaskRunner pid=3489667)[0m Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(TaskRunner pid=3489667)[0m WARNING:2025-12-09 14:18:15,463:Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(TaskRunner pid=3489667)[0m Filtering prompts longer than 2048 tokens (num_proc=1):   0%|          | 0/4 [00:00<?, ? examples/s]
[36m(TaskRunner pid=3489667)[0m Filtering prompts longer than 2048 tokens (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  8.12 examples/s]
[36m(TaskRunner pid=3489667)[0m Filtering prompts longer than 2048 tokens (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.55 examples/s]
[36m(TaskRunner pid=3489667)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=3489667)[0m WARNING:2025-12-09 14:18:17,752:Waiting for register center actor Vwgco8_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=3496357)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3496554)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=3496554)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3496555)[0m Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
[36m(WorkerDict pid=3496556)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3496357)[0m Capturing CUDA graph shapes:   3%|â–Ž         | 1/35 [00:00<00:17,  1.91it/s]
[36m(WorkerDict pid=3496357)[0m Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
[36m(WorkerDict pid=3496357)[0m Capturing CUDA graph shapes:  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:05<00:10,  2.23it/s][32m [repeated 18x across cluster][0m
[36m(WorkerDict pid=3496357)[0m Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:10<00:06,  2.05it/s][32m [repeated 16x across cluster][0m
[36m(WorkerDict pid=3496357)[0m Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:15<00:01,  2.24it/s]
[36m(WorkerDict pid=3496357)[0m Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:15<00:00,  2.26it/s]
[36m(WorkerDict pid=3496555)[0m Capturing CUDA graph shapes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:15<00:09,  1.41it/s][32m [repeated 17x across cluster][0m
[36m(WorkerDict pid=3496357)[0m Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:16<00:00,  2.27it/s]
[36m(WorkerDict pid=3496357)[0m Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:17<00:00,  1.44it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:17<00:00,  2.00it/s]
[36m(WorkerDict pid=3496555)[0m Capturing CUDA graph shapes:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:21<00:05,  1.29it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3496357)[0m /users/3/peng0504/.conda/envs/verl-agent/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3496357)[0m   warnings.warn(
[36m(WorkerDict pid=3496555)[0m Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:24<00:02,  1.37it/s]
[36m(WorkerDict pid=3496555)[0m Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:25<00:01,  1.48it/s]
[36m(WorkerDict pid=3496555)[0m Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:25<00:00,  1.50it/s]
[36m(WorkerDict pid=3496555)[0m Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:26<00:00,  1.26it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:26<00:00,  1.31it/s]
[36m(WorkerDict pid=3496555)[0m Capturing CUDA graph shapes:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:23<00:03,  1.29it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3496556)[0m /users/3/peng0504/.conda/envs/verl-agent/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3496556)[0m   warnings.warn([32m [repeated 2x across cluster][0m
[36m(TaskRunner pid=3489667)[0m wandb: Currently logged in as: peng0504 (mhong-university-of-minnesota) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(TaskRunner pid=3489667)[0m wandb: setting up run d06xu3uo
[36m(WorkerDict pid=3496555)[0m /users/3/peng0504/.conda/envs/verl-agent/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3496555)[0m   warnings.warn(
[36m(TaskRunner pid=3489667)[0m wandb: Tracking run with wandb version 0.23.0
[36m(TaskRunner pid=3489667)[0m wandb: Run data is saved locally in /projects/standard/mhong/peng0504/wandb/run-20251209_141958-d06xu3uo
[36m(TaskRunner pid=3489667)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(TaskRunner pid=3489667)[0m wandb: Syncing run grpo_qwen2.5_0.5b
[36m(TaskRunner pid=3489667)[0m wandb: â­ï¸ View project at https://wandb.ai/mhong-university-of-minnesota/verl_agent_alfworld
[36m(TaskRunner pid=3489667)[0m wandb: ðŸš€ View run at https://wandb.ai/mhong-university-of-minnesota/verl_agent_alfworld/runs/d06xu3uo
[36m(TaskRunner pid=3489667)[0m wandb: Detected [openai] in use.
[36m(TaskRunner pid=3489667)[0m wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[36m(TaskRunner pid=3489667)[0m wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
[36m(TaskRunner pid=3489667)[0m Training Progress:   0%|          | 0/150 [00:00<?, ?it/s]
[36m(TaskRunner pid=3489667)[0m Training Progress:   1%|          | 1/150 [03:36<8:56:55, 216.21s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   1%|â–         | 2/150 [06:47<8:17:25, 201.66s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   2%|â–         | 3/150 [09:41<7:42:38, 188.83s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   3%|â–Ž         | 4/150 [12:20<7:11:19, 177.26s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   3%|â–Ž         | 5/150 [16:05<7:50:02, 194.50s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   4%|â–         | 6/150 [18:40<7:14:27, 181.03s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   5%|â–         | 7/150 [21:15<6:51:15, 172.56s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   5%|â–Œ         | 8/150 [23:41<6:28:18, 164.07s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   6%|â–Œ         | 9/150 [26:35<6:32:53, 167.19s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   7%|â–‹         | 10/150 [30:19<7:10:59, 184.71s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   7%|â–‹         | 11/150 [33:00<6:51:15, 177.52s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   8%|â–Š         | 12/150 [35:34<6:31:51, 170.37s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   9%|â–Š         | 13/150 [38:01<6:12:25, 163.10s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:   9%|â–‰         | 14/150 [40:26<5:57:34, 157.75s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  10%|â–ˆ         | 15/150 [43:58<6:31:19, 173.92s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  11%|â–ˆ         | 16/150 [46:14<6:03:24, 162.72s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  11%|â–ˆâ–        | 17/150 [48:37<5:47:20, 156.69s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  12%|â–ˆâ–        | 18/150 [51:09<5:41:49, 155.37s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  13%|â–ˆâ–Ž        | 19/150 [53:28<5:28:08, 150.30s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  13%|â–ˆâ–Ž        | 20/150 [57:21<6:19:27, 175.14s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  14%|â–ˆâ–        | 21/150 [59:58<6:04:45, 169.65s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  15%|â–ˆâ–        | 22/150 [1:02:16<5:42:01, 160.33s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  15%|â–ˆâ–Œ        | 23/150 [1:04:34<5:24:46, 153.44s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  16%|â–ˆâ–Œ        | 24/150 [1:06:57<5:16:00, 150.48s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  17%|â–ˆâ–‹        | 25/150 [1:10:43<6:00:32, 173.06s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  17%|â–ˆâ–‹        | 26/150 [1:12:59<5:34:55, 162.06s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  18%|â–ˆâ–Š        | 27/150 [1:15:22<5:20:24, 156.29s/it]
[36m(TaskRunner pid=3489667)[0m Training Progress:  19%|â–ˆâ–Š        | 28/150 [1:17:49<5:11:55, 153.41s/it]
